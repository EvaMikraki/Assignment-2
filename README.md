# Machine Learning in Computational Biology: Assignment 2

## Project Overview

This project implements a machine learning pipeline to classify breast cancer tumors as malignant (M) or benign (B) based on features derived from digitized images of fine needle aspirates (FNA). The primary goals were to perform Exploratory Data Analysis (EDA), implement a Repeated Nested Cross-Validation (rnCV) framework to select the best classification algorithm, and train a final model instance for deployment.

## Directory Structure

The project follows this structure:

```
Assignment-2/
├── data/
│   └── breast_cancer.csv               # Input dataset
├── models/
│   └── final_lr_elasticnet_model.pkl   # Final trained model pipeline
├── notebooks/
│   ├── 1_data_exploration.ipynb        # Notebook for Task 1: Exploratory Data Analysis
│   ├── 2_nCV_sketch.ipynb              # Notebook for Task 2: Preparation for the nCV Implementation
│   ├── 3_model_selection.ipynb         # Notebook for Task 3: rnCV Implementation and Winner Model Selection
│   ├── 4_final_model_training.ipynb    # Notebook for Task 4: Final Model Training 
│   └── nCV_sketch.png                  # Diagram for Task 2: Sketch of one iteration of nCV   
├── src/
│   ├── rnCV.py                         # Python class for rnCV
│   └── predict.py                      # Script to make predictions using the final model
├── README.md                           # This file
└── requirements.txt                    # Python package requirements
```

## Setup

1.  **Clone the repository:**
    ```bash
    git clone <your-repo-url>
    cd Assignment-2
    ```
2.  **Create a Python Environment:** It's recommended to use a virtual environment (e.g., conda or venv).
    ```bash
    # Example using conda
    conda create -n ml_assignment2 python=3.9 # Or your preferred python version
    conda activate ml_assignment2
    ```
3.  **Install Dependencies:** Install the required Python packages using the provided `requirements.txt` file:
    ```bash
    pip install -r requirements.txt
    ```

## Usage

### 1. Running the Analysis

The analysis is separated into notebooks in the `notebooks/` directory:

* **EDA (`1_data_exploration.ipynb`):** Explores the dataset (Task 1).
* **Model Selection (`3_model_selection.ipynb`):** Runs the Repeated Nested Cross-Validation using `src/rnCV.py` to compare models and select the winner (Task 3). *Note: This can take ~25 minutes to run with the simplified grids.* Results are saved to `rnCV_results.csv` and `rnCV_summary_stats_CIs.csv`.
* **Final Training (`4_final_model_training.ipynb`):** Takes the winner algorithm, performs final hyperparameter tuning using GridSearchCV, trains the final model pipeline on all data and saves it to `models/final_lr_elasticnet_model.pkl` (Task 4).

*(You can run these notebooks sequentially using Jupyter Lab or Jupyter Notebook)*

### 2. Making Predictions on New Data

Use the `predict.py` script to generate predictions using the final trained model.

**Prerequisites:**
* The final model must exist at `models/final_lr_elasticnet_model.pkl` (generated by running the `4_final_model_training.ipynb` notebook).
* You need an input data file (CSV format) containing the 30 required features with the exact same column names as used in training. An optional 'id' column can be present.

**Command (run from the `Assignment-2` root directory):**

```bash
python src/predict.py --model_path models/final_lr_elasticnet_model.pkl --data_path /path/to/your/input_data.csv --output_path /path/to/your/output_predictions.csv
```

**Replace:**
* `/path/to/your/input_data.csv` with the actual path to the new data file (e.g., your hold-out set).
* `/path/to/your/output_predictions.csv` with the desired path to save the resulting predictions.

The output CSV will contain the predictions (column named `prediction`) and the 'id' column if it was present in the input data.
